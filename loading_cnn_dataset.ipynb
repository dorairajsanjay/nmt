{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CNN Data and reducing large vocab to a smaller sized vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CNN data\n",
    "\n",
    "This code is adapted from https://machinelearningmastery.com/prepare-news-articles-text-summarization/ for the purpose of loading data from CNN dataset.\n",
    "\n",
    "You can download CNN data from https://cs.nyu.edu/~kcho/DMQA/\n",
    "\n",
    "Note the below directory for the downloaded files\n",
    "directory = 'data/cnn/stories/'\n",
    "\n",
    "The last cell in this notebook converts the CNN dataset into a format that can be consumed by the NMT model at the below URL \n",
    "\n",
    "https://github.com/tensorflow/nmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Stories 92579\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "import string\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# split a document into news story and highlights\n",
    "def split_story(doc):\n",
    "\t# find first highlight\n",
    "\tindex = doc.find('@highlight')\n",
    "\t# split into story and highlights\n",
    "\tstory, highlights = doc[:index], doc[index:].split('@highlight')\n",
    "\t# strip extra white space around each highlight\n",
    "\thighlights = [h.strip() for h in highlights if len(h) > 0]\n",
    "\treturn story, highlights\n",
    "\n",
    "# load all stories in a directory\n",
    "def load_stories(directory):\n",
    "\tstories = list()\n",
    "\tfor name in listdir(directory):\n",
    "\t\tfilename = directory + '/' + name\n",
    "\t\t# load document\n",
    "\t\tdoc = load_doc(filename)\n",
    "\t\t# split into story and highlights\n",
    "\t\tstory, highlights = split_story(doc)\n",
    "\t\t# store\n",
    "\t\tstories.append({'story':story, 'highlights':highlights})\n",
    "\treturn stories\n",
    "\n",
    "# clean a list of lines\n",
    "def clean_lines(lines):\n",
    "\tcleaned = list()\n",
    "\t# prepare a translation table to remove punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor line in lines:\n",
    "\t\t# strip source cnn office if it exists\n",
    "\t\tindex = line.find('(CNN) -- ')\n",
    "\t\tif index > -1:\n",
    "\t\t\tline = line[index+len('(CNN)'):]\n",
    "\t\t# tokenize on white space\n",
    "\t\tline = line.split()\n",
    "\t\t# convert to lower case\n",
    "\t\tline = [word.lower() for word in line]\n",
    "\t\t# remove punctuation from each token\n",
    "\t\tline = [w.translate(table) for w in line]\n",
    "\t\t# remove tokens with numbers in them\n",
    "\t\tline = [word for word in line if word.isalpha()]\n",
    "\t\t# store as string\n",
    "\t\tcleaned.append(' '.join(line))\n",
    "\t# remove empty strings\n",
    "\tcleaned = [c for c in cleaned if len(c) > 0]\n",
    "\treturn cleaned\n",
    "\n",
    "# load stories\n",
    "directory = '/tmp/mnt_fork/tldr_data/cnn/stories'\n",
    "stories = load_stories(directory)\n",
    "print('Loaded Stories %d' % len(stories))\n",
    "\n",
    "# clean stories\n",
    "for example in stories:\n",
    "\texample['story'] = clean_lines(example['story'].split('\\n'))\n",
    "\texample['highlights'] = clean_lines(example['highlights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving data to a pickle file for later use\n",
    "\n",
    "# save to file\n",
    "from pickle import dump\n",
    "dump(stories, open('cnn_dataset.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Stories 92579\n"
     ]
    }
   ],
   "source": [
    "#### Loading CNN dataset\n",
    "\n",
    "import pickle\n",
    "\n",
    "pFile = open('cnn_dataset.pkl', 'rb')\n",
    "\n",
    "pFile.seek(0)\n",
    "\n",
    "stories = pickle.load(pFile)\n",
    "print('Loaded Stories %d' % len(stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(stories,train_pct,dev_pct,test_pct):\n",
    "    \n",
    "    '''\n",
    "    Input:\n",
    "    \n",
    "    stories  : list of stories. Each element is a dictionar with two keys and corresponding values.\n",
    "               key1 is \"highlight\" and key2 is story. \n",
    "               \"highlight\" is a list of highlights or summary of the story spread across multiple lines\n",
    "               \"story\" is the news story spread across multiple lines\n",
    "    train_pct: floating point number. Percentage to dedicate for training\n",
    "    dev_pct  : floating point number. Percentage to dedicate for dev testset\n",
    "    test_pct : floating point number. Percentage to dedicate for testing\n",
    "    \n",
    "    Output:\n",
    "    \n",
    "    train_in, train_out, dev_in, dev_out, test_in, test_out: \n",
    "                corresponding to training and test input and output sentences.\n",
    "                All sentences are squashed to a single line\n",
    "    '''\n",
    "    \n",
    "    data_in = []\n",
    "    data_out = []\n",
    "    \n",
    "    allHighlights = False\n",
    "    \n",
    "    for story in stories:\n",
    "        \n",
    "        if allHighlights == False:\n",
    "            summary = story[\"highlights\"][0]\n",
    "        else:\n",
    "            summary = \".\".join(story[\"highlights\"])\n",
    "        details = \".\".join(story[\"story\"])\n",
    "        \n",
    "        data_in.append(details)\n",
    "        data_out.append(summary)\n",
    "\n",
    "    # splice list\n",
    "    train_end  = int(len(data_in) * abs(float(train_pct)/100))\n",
    "    dev_begin  = train_end # to adjust for python being zero based\n",
    "    dev_end    = int(len(data_in) * abs(float(train_pct+dev_pct)/100))\n",
    "    test_begin = dev_end\n",
    "    \n",
    "    train_in  = data_in[0:train_end]\n",
    "    train_out = data_out[0:train_end]\n",
    "    \n",
    "    dev_in = data_in[dev_begin:dev_end]\n",
    "    dev_out = data_out[dev_begin:dev_end]\n",
    "    \n",
    "    test_in = data_in[test_begin:]\n",
    "    test_out = data_out[test_begin:]\n",
    "    \n",
    "    return train_in, train_out, dev_in, dev_out, test_in, test_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_in, train_out, dev_in, dev_out, test_in, test_out = getData(stories,90,5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_in:83321, train_out:83321, dev_in:4629, dev_out:4629, test_in:4629, test_out:4629\n"
     ]
    }
   ],
   "source": [
    "print(\"train_in:%d, train_out:%d, dev_in:%d, dev_out:%d, test_in:%d, test_out:%d\" % (len(train_in), len(train_out), len(dev_in), len(dev_out),len(test_in), len(test_out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data out to file\n",
    "\n",
    "dest_dir = \"/tmp/mnt_fork/tldr_data\"\n",
    "\n",
    "with open(dest_dir+\"/train.in\",\"w\") as outfile:\n",
    "    outfile.write('\\n'.join(train_in))\n",
    "    \n",
    "with open(dest_dir+\"/train.out\",\"w\") as outfile:\n",
    "    outfile.write('\\n'.join(train_out))\n",
    "    \n",
    "with open(dest_dir+\"/dev.in\",\"w\") as outfile:\n",
    "    outfile.write('\\n'.join(dev_in))\n",
    "    \n",
    "with open(dest_dir+\"/dev.out\",\"w\") as outfile:\n",
    "    outfile.write('\\n'.join(dev_out))\n",
    "    \n",
    "with open(dest_dir+\"/test.in\",\"w\") as outfile:\n",
    "    outfile.write('\\n'.join(test_in))\n",
    "    \n",
    "with open(dest_dir+\"/test.out\",\"w\") as outfile:\n",
    "    outfile.write('\\n'.join(test_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract a reduced sized vocabulary from data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "stories_dataset = [item[\"story\"] for item in stories]\n",
    "highlights_dataset = [item[\"highlights\"][0] for item in stories]\n",
    "\n",
    "stories_dataset_final = list(map(lambda x: \".\".join(x),stories_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create stories vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "# write these 20k into vocab\n",
    "vocabSize = 20000\n",
    "\n",
    "all_tokens = []\n",
    "\n",
    "for story in stories_dataset_final:\n",
    "    \n",
    "    tokens = nltk.word_tokenize(story)\n",
    "    \n",
    "    all_tokens.extend(tokens)\n",
    "    \n",
    "counts = Counter(all_tokens)\n",
    "\n",
    "# get most common words\n",
    "common_words = counts.most_common(vocabSize)\n",
    "\n",
    "# write to output file\n",
    "with open(\"vocab_20k.in\",\"w\") as vocabFile:\n",
    "    for i in range(0,vocabSize):\n",
    "        vocabFile.write(common_words[i][0])\n",
    "        vocabFile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create headlines vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92579\n"
     ]
    }
   ],
   "source": [
    "print(len(highlights_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "# write these 20k into vocab\n",
    "vocabSize = 20000\n",
    "\n",
    "all_tokens = []\n",
    "\n",
    "for highlight in highlights_dataset:\n",
    "    \n",
    "    tokens = nltk.word_tokenize(highlight)\n",
    "    \n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "counts = Counter(all_tokens)\n",
    "\n",
    "# get most common words\n",
    "common_words = counts.most_common(vocabSize)\n",
    "\n",
    "# write to output file\n",
    "with open(\"vocab_20k.out\",\"w\") as vocabFile:\n",
    "    for i in range(0,vocabSize):\n",
    "        vocabFile.write(common_words[i][0])\n",
    "        vocabFile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing VOCAB size\n",
    "\n",
    "This notebook looks at the CNN dataset vocab files available at the below link\n",
    "http://cs.nyu.edu/~kcho/DMQA/\n",
    "\n",
    "the vocab file is in the format\n",
    "\n",
    "    word1 frequency1\n",
    "    word2 frequency2\n",
    "    word3 frequency3\n",
    "    word4 frequency4\n",
    "    ...\n",
    "\n",
    "We filter out words with low frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inFile = \"vocab\"\n",
    "outFile = \"vocab.out\"\n",
    "frequencyCutOff = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outVocab = []\n",
    "with open(inFile) as readFile:\n",
    "    \n",
    "    line = readFile.readline()\n",
    "    \n",
    "    while line != \"\":\n",
    "\n",
    "        tokens = line.split()\n",
    "\n",
    "        if len(tokens) == 2 and int(tokens[1]) > frequencyCutOff:\n",
    "            outVocab.append(tokens[0])\n",
    "            \n",
    "        line = readFile.readline()\n",
    "        \n",
    "        \n",
    "with open(outFile,\"w\") as writeFile:\n",
    "    \n",
    "    writeFile.write(\"\\n\".join(outVocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36plus)",
   "language": "python",
   "name": "conda_tensorflow_p36plus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
