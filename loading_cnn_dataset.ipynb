{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CNN Data and reducing large vocab to a smaller sized vocab\n",
    "\n",
    "You can download CNN data from https://cs.nyu.edu/~kcho/DMQA/\n",
    "\n",
    "Note the below directory for the downloaded files\n",
    "directory = 'data/cnn/stories/'\n",
    "\n",
    "The last cell in this notebook converts the CNN dataset into a format that can be consumed by the NMT model at the below URL \n",
    "\n",
    "https://github.com/tensorflow/nmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CNN data\n",
    "\n",
    "This code is adapted from https://machinelearningmastery.com/prepare-news-articles-text-summarization/ for the purpose of loading data from CNN dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Stories 92579\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "import string\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# split a document into news story and highlights\n",
    "def split_story(doc):\n",
    "\t# find first highlight\n",
    "\tindex = doc.find('@highlight')\n",
    "\t# split into story and highlights\n",
    "\tstory, highlights = doc[:index], doc[index:].split('@highlight')\n",
    "\t# strip extra white space around each highlight\n",
    "\thighlights = [h.strip() for h in highlights if len(h) > 0]\n",
    "\treturn story, highlights\n",
    "\n",
    "# load all stories in a directory\n",
    "def load_stories(directory):\n",
    "\tstories = list()\n",
    "\tfor name in listdir(directory):\n",
    "\t\tfilename = directory + '/' + name\n",
    "\t\t# load document\n",
    "\t\tdoc = load_doc(filename)\n",
    "\t\t# split into story and highlights\n",
    "\t\tstory, highlights = split_story(doc)\n",
    "\t\t# store\n",
    "\t\tstories.append({'story':story, 'highlights':highlights})\n",
    "\treturn stories\n",
    "\n",
    "# clean a list of lines\n",
    "def clean_lines(lines):\n",
    "\tcleaned = list()\n",
    "\t# prepare a translation table to remove punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor line in lines:\n",
    "\t\t# strip source cnn office if it exists\n",
    "\t\tindex = line.find('(CNN) -- ')\n",
    "\t\tif index > -1:\n",
    "\t\t\tline = line[index+len('(CNN)'):]\n",
    "\t\t# tokenize on white space\n",
    "\t\tline = line.split()\n",
    "\t\t# convert to lower case\n",
    "\t\tline = [word.lower() for word in line]\n",
    "\t\t# remove punctuation from each token\n",
    "\t\tline = [w.translate(table) for w in line]\n",
    "\t\t# remove tokens with numbers in them\n",
    "\t\tline = [word for word in line if word.isalpha()]\n",
    "\t\t# store as string\n",
    "\t\tcleaned.append(' '.join(line))\n",
    "\t# remove empty strings\n",
    "\tcleaned = [c for c in cleaned if len(c) > 0]\n",
    "\treturn cleaned\n",
    "\n",
    "# load stories\n",
    "directory = '/tmp/mnt_fork/tldr_data/cnn/stories'\n",
    "stories = load_stories(directory)\n",
    "print('Loaded Stories %d' % len(stories))\n",
    "\n",
    "# clean stories\n",
    "for example in stories:\n",
    "\texample['story'] = clean_lines(example['story'].split('\\n'))\n",
    "\texample['highlights'] = clean_lines(example['highlights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and restoring data from a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving data to a pickle file for later use\n",
    "\n",
    "# save to file\n",
    "from pickle import dump\n",
    "dump(stories, open('cnn_dataset.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Stories 92579\n"
     ]
    }
   ],
   "source": [
    "#### Loading CNN dataset\n",
    "\n",
    "import pickle\n",
    "\n",
    "pFile = open('cnn_dataset.pkl', 'rb')\n",
    "\n",
    "pFile.seek(0)\n",
    "\n",
    "stories = pickle.load(pFile)\n",
    "print('Loaded Stories %d' % len(stories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training, dev and test datasets\n",
    "\n",
    "Note the flag \"allHighlights = False\" below.\n",
    "\n",
    "If this is set to True, then all highlights are joined with a period separator to create a larger summary as the target data. If set to False, then only the first highlight is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(stories,train_pct,dev_pct,test_pct):\n",
    "    \n",
    "    '''\n",
    "    Input:\n",
    "    \n",
    "    stories  : list of stories. Each element is a dictionar with two keys and corresponding values.\n",
    "               key1 is \"highlight\" and key2 is story. \n",
    "               \"highlight\" is a list of highlights or summary of the story spread across multiple lines\n",
    "               \"story\" is the news story spread across multiple lines\n",
    "    train_pct: floating point number. Percentage to dedicate for training\n",
    "    dev_pct  : floating point number. Percentage to dedicate for dev testset\n",
    "    test_pct : floating point number. Percentage to dedicate for testing\n",
    "    \n",
    "    Output:\n",
    "    \n",
    "    train_in, train_out, dev_in, dev_out, test_in, test_out: \n",
    "                corresponding to training and test input and output sentences.\n",
    "                All sentences are squashed to a single line\n",
    "    '''\n",
    "    \n",
    "    data_in = []\n",
    "    data_out = []\n",
    "    \n",
    "    allHighlights = False\n",
    "    \n",
    "    for story in stories:\n",
    "        \n",
    "        if allHighlights == False:\n",
    "            summary = story[\"highlights\"][0]\n",
    "        else:\n",
    "            summary = \".\".join(story[\"highlights\"])\n",
    "        details = \".\".join(story[\"story\"])\n",
    "        \n",
    "        # remove empty lines\n",
    "        if details == \"\":\n",
    "            continue\n",
    "        \n",
    "        # add to dataset\n",
    "        data_in.append(details)\n",
    "        data_out.append(summary)\n",
    "\n",
    "    # splice list\n",
    "    train_end  = int(len(data_in) * abs(float(train_pct)/100))\n",
    "    dev_begin  = train_end # to adjust for python being zero based\n",
    "    dev_end    = int(len(data_in) * abs(float(train_pct+dev_pct)/100))\n",
    "    test_begin = dev_end\n",
    "    \n",
    "    train_in  = data_in[0:train_end]\n",
    "    train_out = data_out[0:train_end]\n",
    "    \n",
    "    dev_in = data_in[dev_begin:dev_end]\n",
    "    dev_out = data_out[dev_begin:dev_end]\n",
    "    \n",
    "    test_in = data_in[test_begin:]\n",
    "    test_out = data_out[test_begin:]\n",
    "    \n",
    "    return train_in, train_out, dev_in, dev_out, test_in, test_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_in:83218, train_out:83218, dev_in:4623, dev_out:4623, test_in:4624, test_out:4624\n"
     ]
    }
   ],
   "source": [
    "# invoke method to get training, dev and test set at 90:5:5 proporation\n",
    "train_in, train_out, dev_in, dev_out, test_in, test_out = getData(stories,90,5,5)\n",
    "print(\"train_in:%d, train_out:%d, dev_in:%d, dev_out:%d, test_in:%d, test_out:%d\" % (len(train_in), len(train_out), len(dev_in), len(dev_out),len(test_in), len(test_out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data out to file\n",
    "\n",
    "dest_dir = \"/tmp/mnt_fork/tldr_data\"\n",
    "\n",
    "with open(dest_dir+\"/train.in\",\"w\") as outfile:\n",
    "    outfile.write('\\n'.join(train_in))\n",
    "    \n",
    "with open(dest_dir+\"/train.out\",\"w\") as outfile:\n",
    "    outfile.write('\\n'.join(train_out))\n",
    "    \n",
    "with open(dest_dir+\"/dev.in\",\"w\") as outfile:\n",
    "    outfile.write('\\n'.join(dev_in))\n",
    "    \n",
    "with open(dest_dir+\"/dev.out\",\"w\") as outfile:\n",
    "    outfile.write('\\n'.join(dev_out))\n",
    "    \n",
    "with open(dest_dir+\"/test.in\",\"w\") as outfile:\n",
    "    outfile.write('\\n'.join(test_in))\n",
    "    \n",
    "with open(dest_dir+\"/test.out\",\"w\") as outfile:\n",
    "    outfile.write('\\n'.join(test_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract a reduced sized vocabulary from data\n",
    "\n",
    "This section allows you to extract the vocabulary from this data.\n",
    "\n",
    "Note the \"vocabSize\" variable. You could change this to a much higher number depending on the resources you have available to you. I have set it to 20000 by default\n",
    "\n",
    "Also, note also \"vocabFileName\", the name of the vocab file.\n",
    "\n",
    "There are two sections here. One sectionc creates the vocabulary for all the stories and the other section creates the vocabulary for headlines. Understood it is not optimal but maybe I will go back and tweak this later to make it more optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "stories_dataset = [item[\"story\"] for item in stories]\n",
    "highlights_dataset = [item[\"highlights\"][0] for item in stories]\n",
    "\n",
    "stories_dataset_final = list(map(lambda x: \".\".join(x),stories_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create stories vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization in progress. Completed:2000. Remaining:90579\n",
      "Tokenization in progress. Completed:4000. Remaining:88579\n",
      "Tokenization in progress. Completed:6000. Remaining:86579\n",
      "Tokenization in progress. Completed:8000. Remaining:84579\n",
      "Tokenization in progress. Completed:10000. Remaining:82579\n",
      "Tokenization in progress. Completed:12000. Remaining:80579\n",
      "Tokenization in progress. Completed:14000. Remaining:78579\n",
      "Tokenization in progress. Completed:16000. Remaining:76579\n",
      "Tokenization in progress. Completed:18000. Remaining:74579\n",
      "Tokenization in progress. Completed:20000. Remaining:72579\n",
      "Tokenization in progress. Completed:22000. Remaining:70579\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "# write these 20k into vocab\n",
    "vocabSize = 20000\n",
    "vocabFileName = \"vocab_20k.in\"\n",
    "\n",
    "all_tokens = []\n",
    "\n",
    "count = 0\n",
    "for story in stories_dataset_final:\n",
    "    \n",
    "    tokens = nltk.word_tokenize(story)\n",
    "    \n",
    "    all_tokens.extend(tokens)\n",
    "    \n",
    "    count += 1\n",
    "    if count%2000 == 0:\n",
    "        print(\"Tokenization in progress. Completed:%d. Remaining:%d\" % (count,len(stories_dataset_final)-count))\n",
    "    \n",
    "counts = Counter(all_tokens)\n",
    "\n",
    "# get most common words\n",
    "common_words = counts.most_common(vocabSize)\n",
    "\n",
    "# write to output file\n",
    "with open(vocabFileName,\"w\") as vocabFile:\n",
    "    for i in range(0,vocabSize):\n",
    "        vocabFile.write(common_words[i][0])\n",
    "        vocabFile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create headlines vocabulary\n",
    "\n",
    "You need to set the vocabSize and vocabFileName again here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(highlights_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "# write these 20k into vocab\n",
    "vocabSize = 20000\n",
    "vocabFileName = \"vocab_20k.out\"\n",
    "\n",
    "all_tokens = []\n",
    "\n",
    "for highlight in highlights_dataset:\n",
    "    \n",
    "    tokens = nltk.word_tokenize(highlight)\n",
    "    \n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "counts = Counter(all_tokens)\n",
    "\n",
    "# get most common words\n",
    "common_words = counts.most_common(vocabSize)\n",
    "\n",
    "# write to output file\n",
    "\n",
    "with open(vocabFileName,\"w\") as vocabFile:\n",
    "    for i in range(0,vocabSize):\n",
    "        vocabFile.write(common_words[i][0])\n",
    "        vocabFile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing VOCAB size\n",
    "\n",
    "This is some optional code in case you already have a vocabulary available with words and their relative frequencies.\n",
    "\n",
    "for example, the vocab file is in the format\n",
    "\n",
    "    word1 frequency1\n",
    "    word2 frequency2\n",
    "    word3 frequency3\n",
    "    word4 frequency4\n",
    "    ...\n",
    "\n",
    "We filter out words with low frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inFile = \"vocab\"\n",
    "outFile = \"vocab.out\"\n",
    "frequencyCutOff = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outVocab = []\n",
    "with open(inFile) as readFile:\n",
    "    \n",
    "    line = readFile.readline()\n",
    "    \n",
    "    while line != \"\":\n",
    "\n",
    "        tokens = line.split()\n",
    "\n",
    "        if len(tokens) == 2 and int(tokens[1]) > frequencyCutOff:\n",
    "            outVocab.append(tokens[0])\n",
    "            \n",
    "        line = readFile.readline()\n",
    "        \n",
    "        \n",
    "with open(outFile,\"w\") as writeFile:\n",
    "    \n",
    "    writeFile.write(\"\\n\".join(outVocab))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36plus)",
   "language": "python",
   "name": "conda_tensorflow_p36plus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
